In [1] :

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import matplotlib.image as mpimg

In [2] : 

# Downloading the zipfile and extracting

!gdown 11R-D1robYjdyMZ4KooWuAlZH1WyRpCdl
     
Downloading...
From (original): https://drive.google.com/uc?id=11R-D1robYjdyMZ4KooWuAlZH1WyRpCdl
From (redirected): https://drive.google.com/uc?id=11R-D1robYjdyMZ4KooWuAlZH1WyRpCdl&confirm=t&uuid=c5890823-3393-4310-94f2-e16ceb5b7957
To: /content/BrainTumor_1.zip
100% 514M/514M [00:15<00:00, 32.5MB/s]

In [3] :


# Unzip the file
import zipfile

zip_ref = zipfile.ZipFile("BrainTumor_1.zip")
zip_ref.extractall()
zip_ref.close()


In [4] :


# Lets store our training and testing directories

train_dir = "/content/BrainTumor_1/Train"
test_dir = "/content/BrainTumor_1/Test"
     
In [5] :


# Plotting the number of images of each category
import os

glioma_count = len(os.listdir("/content/BrainTumor_1/Train/glioma"))
meningioma_count = len(os.listdir("/content/BrainTumor_1/Train/meningioma"))
pituitary_count = len(os.listdir("/content/BrainTumor_1/Train/notumor"))
no_tumor_count = len(os.listdir("/content/BrainTumor_1/Train/pituitary"))

plt.bar(["glioma","meningioma","notumour","pituitary"],[glioma_count,meningioma_count,no_tumor_count,pituitary_count])
plt.title("Number of images of each category")
plt.xlabel("Category")
plt.ylabel("Number of images")

plt.text(0,glioma_count,s=glioma_count,fontweight='bold',fontsize='medium',ha='center',va='bottom')
plt.text(1,meningioma_count,s=meningioma_count,fontweight='bold',fontsize='medium',ha='center',va='bottom')
plt.text(2,no_tumor_count,s=no_tumor_count,fontweight='bold',fontsize='medium',ha='center',va='bottom')
plt.text(3,pituitary_count,s=pituitary_count,fontweight='bold',fontsize='medium',ha='center',va='bottom')

Out [5] :
Text(3, 6380, '6380')








In [6] :


# Testing the shape of one image to understand the dimensions

image = mpimg.imread("/content/BrainTumor_1/Train/meningioma/0200.jpg")
plt.imshow(image)
image.shape

Out [6] :

(345, 300, 3)












In [7] : 


# Getting the data loaders ready to load data into variables

from tensorflow.keras.preprocessing import image_dataset_from_directory


train_data,validation_data = image_dataset_from_directory(directory=train_dir,
                                          label_mode='int',
                                          color_mode='grayscale',
                                          batch_size=32,
                                          image_size=(128,128),
                                          seed=42,
                                          subset="both",
                                          validation_split=0.2,
                                          shuffle=True)

test_data = image_dataset_from_directory(directory=test_dir,
                                          label_mode='int',
                                          color_mode='grayscale',
                                          batch_size=32,
                                          image_size=(128,128),
                                          seed=42,
                                          shuffle=True)
     
Found 22848 files belonging to 4 classes.
Using 18279 files for training.
Using 4569 files for validation.
Found 1311 files belonging to 4 classes.


In [8] :


# Lets store the class names to access them

class_names = train_data.class_names
class_names

 Out [8] :

['glioma', 'meningioma', 'notumor', 'pituitary']


In [9] :

import numpy as np

# Initialize counts
n_glioma = 0
n_meningioma = 0
n_notumor = 0
n_pituitary = 0


for images, labels in train_data:
    labels_np = labels.numpy()


    for label in labels_np:
        if class_names[int(label)] == 'glioma':
            n_glioma += 1
        elif class_names[int(label)] == 'meningioma':
            n_meningioma += 1
        elif class_names[int(label)] == 'notumor':
            n_notumor += 1
        elif class_names[int(label)] == 'pituitary':
            n_pituitary += 1


print("Number of samples for each class:")
print("Glioma:", n_glioma)
print("Meningioma:", n_meningioma)
print("No Tumor:", n_notumor)
print("Pituitary:", n_pituitary)

print(n_glioma+n_meningioma+n_notumor+n_pituitary)


Number of samples for each class:
Glioma: 4230
Meningioma: 4332
No Tumor: 5098
Pituitary: 4619
18279



In [10] :


# Calculate the weights of each class to deal wiht class imbalances

glioma_weight = 0
meningioma_weight = 0
notumor_weight = 0
pituitary_weight =0
n = 18279

class_count = np.array([4230,4332,5098,4619])

class_weights = {0:glioma_weight,1:meningioma_weight,2:notumor_weight,3:pituitary_weight}

for i in range(4):
    class_weights[i] = (n/(4*class_count[i]))

class_weights


Out [10] :

{0: 1.0803191489361703,
 1: 1.054882271468144,
 2: 0.89638093369949,
 3: 0.9893375189434943}




In [11] :


import matplotlib.pyplot as plt

samples_count = [4230, 4332, 5098, 4619]
samples_percentage = []
total = sum(samples_count)

for i in samples_count:
    k = i / total
    samples_percentage.append(k)

plt.figure(figsize=(6, 6))
plt.pie(
    samples_count,
    labels=["Glioma", "Meningioma", "No Tumour", "Pituitary"],
    autopct=lambda p: f'{p:.2f}%',
    pctdistance=0.5,
    labeldistance=1.1,
    colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
)

plt.legend(["Glioma", "Meningioma", "No Tumour", "Pituitary"], bbox_to_anchor=(1.1, 1.1))

plt.show()
















In [12] :


# Lets take an image and check its shape

for image,labels in train_data.take(1):
  plt.imshow(image[0]) #view the first image of the batch
  print(labels[0]) #view label of first image
  plt.title(f"{class_names[labels[0]]}")
  print(image[0].shape)


tf.Tensor(1, shape=(), dtype=int32)
(128, 128, 1)


















# Create a function to view multiple images at once

def view_images(train_data):
  for images,labels in train_data.take(1):
    plt.figure()
    for i in range(4):
      plt.subplot(2,2,i+1)
      plt.imshow(images[i])
      plt.title(f"{class_names[labels[i]]}")
      plt.xlabel(f"{images[i].shape}")
    plt.tight_layout()



In [13] :


# Check the min and max values of image vector(if needed rescale)
for images,labels in train_data.take(1):
  print(tf.reduce_min(images[0]))
  print(tf.reduce_max(images[0]))
     
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(255.0, shape=(), dtype=float32)



In [14] :


# Create a custom function and combine with tensorflow function to convert batches of images to rgb

def grayscale_to_rgb(images,labels):
  images = tf.image.grayscale_to_rgb(images)
  return images,labels

train_data = train_data.map(grayscale_to_rgb)
validation_data = validation_data.map(grayscale_to_rgb)
test_data = test_data.map(grayscale_to_rgb)



In [16] :


# Lets check if our images are succesfully converted to rgb
view_images(train_data)


WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
  














In [17] :


# Lets check the min and max values of the images

for images,labels in train_data.take(1):
  print(tf.reduce_min(images[0]))
  print(tf.reduce_max(images[0]))
     
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(255.0, shape=(), dtype=float32)



In [18] :


# Lets retrieve number of batches

len(train_data)

Out [18] :

572

In [19] :


from tensorflow.keras import layers
# create a  rescaling layer

rescale_layer = layers.Rescaling(1/255.)
     
In [20] :


# Lets rescale the images batch-wise

def rescale_over_batches(images,labels):
  images = rescale_layer(images)
  return images,labels

train_data = train_data.map(rescale_over_batches)
validation_data = validation_data.map(rescale_over_batches)
test_data = test_data.map(rescale_over_batches)
     
In [21] :


for images,labels in train_data.take(1):
  print(tf.reduce_min(images[0]))
  print(tf.reduce_max(images[0]))


tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(0.9048564, shape=(), dtype=float32)



In [22] :


# Since rescaling is done lets again try to view the images

view_images(train_data)














MODEL BUILDING


EfficientNet V2B2

In [23] :


from tensorflow.keras.applications import EfficientNetV2B2

base_model = EfficientNetV2B2(include_top=False)
base_model.trainable = False
     
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b2_notop.h5
35839040/35839040 ━━━━━━━━━━━━━━━━━━━━ 3s 0us/step


In [24] :


import random
tf.random.set_seed(42)
np.random.seed(42)
random.seed(42)
# Create an input layer
inputs = tf.keras.layers.Input(shape=(128,128,3))

# Preprocess layer of EfficientNet
tf.keras.applications.efficientnet_v2.preprocess_input(inputs)

# Import the EfficientNet  pre-trained model
x = base_model(inputs,training=False)

# Lets introduce a AveragePooling layer to redduce the dimensions
x = layers.GlobalAvgPool2D()(x)

# Add a FC layer
x = layers.Dense(1024,kernel_regularizer='l2')(x)
x = layers.Dense(512,kernel_regularizer='l2')(x)
x = layers.Dense(256,kernel_regularizer='l2')(x)

# Add batch normalization
x = layers.BatchNormalization()(x)

#Add actiavtion layer
x = tf.keras.activations.relu(x)

#Add dropout layer
x = layers.Dropout(0.6)(x)

# Lets define the output layer
outputs = layers.Dense(4,activation='softmax')(x)

# Build the model
model = tf.keras.Model(inputs,outputs)


In [25] : 

model.summary()


Model: "functional"

| ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_1 (InputLayer)           │ (None, 128, 128, 3)         │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ efficientnetv2-b2 (Functional)       │ (None, 4, 4, 1408)          │       8,769,374 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ global_average_pooling2d             │ (None, 1408)                │               0 │
│ (GlobalAveragePooling2D)             │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (Dense)                        │ (None, 1024)                │       1,442,816 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (Dense)                      │ (None, 512)                 │         524,800 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_2 (Dense)                      │ (None, 256)                 │         131,328 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ batch_normalization                  │ (None, 256)                 │           1,024 │
│ (BatchNormalization)                 │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ re_lu (ReLU)                         │ (None, 256)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout (Dropout)                    │ (None, 256)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_3 (Dense)                      │ (None, 4)                   │           1,028 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

Total params: 10,870,370 (41.47 MB)
Trainable params: 2,100,484 (8.01 MB)
Non-trainable params: 8,769,886 (33.45 MB)


In [26] : 

def plot_loss_curves(history):
  train_loss = history.history["loss"]
  val_loss = history.history["val_loss"]

  train_accuracy = history.history["accuracy"]
  val_accuracy = history.history["val_accuracy"]

  plt.figure()
  plt.subplot(2,1,1)
  plt.title("Loss curves")
  plt.plot(train_loss,label="Training Loss")
  plt.plot(val_loss,label="Validation Loss")
  plt.xlabel("Epochs")
  plt.ylabel("loss")
  plt.legend()


  plt.subplot(2,1,2)
  plt.title("Accuracy curves")
  plt.plot(train_accuracy,label="Training Accuracy")
  plt.plot(val_accuracy,label="Validation Accuracy")
  plt.xlabel("Epochs")
  plt.ylabel("Accuracy")
  plt.legend()
  plt.tight_layout()


In [27] :

len(base_model.layers)

Out [27] :

349

In [28] :


# Unfreeze layers to fine tune

base_model.trainable = False

for layer in base_model.layers[-310:]:
  layer.trainable=True
     
In [29] :

# Recompile the model
model.compile(loss="sparse_categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
               metrics=["accuracy"])
     
In [30] :

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=2,
                                                          factor=0.1,
                                                          min_lr=1e-8)

checkpoint_path = "EfficientNetV2B2_fine_tune.weights.h5"
checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                save_best_only=True,
                                                save_weights_only=True,
                                                monitor='val_loss')
csv_logger = tf.keras.callbacks.CSVLogger("EfficientNetV2B2_fine_tune.csv")


In [31] : 


# Fit the fine tuned model
history = model.fit(train_data,epochs=25,
                                 validation_data=validation_data,
                                callbacks=[reduce_lr,checkpoint,csv_logger],
                                class_weight=class_weights)
Epoch 1/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 209s 188ms/step - accuracy: 0.5655 - loss: 9.9608 - val_accuracy: 0.8116 - val_loss: 0.6699 - learning_rate: 0.0010
Epoch 2/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 45s 79ms/step - accuracy: 0.7913 - loss: 0.6664 - val_accuracy: 0.8774 - val_loss: 0.4461 - learning_rate: 0.0010
Epoch 3/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 45s 78ms/step - accuracy: 0.8474 - loss: 0.4931 - val_accuracy: 0.8875 - val_loss: 0.4061 - learning_rate: 0.0010
Epoch 4/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 44s 77ms/step - accuracy: 0.8910 - loss: 0.4062 - val_accuracy: 0.9092 - val_loss: 0.3063 - learning_rate: 0.0010
Epoch 5/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 46s 80ms/step - accuracy: 0.9174 - loss: 0.3163 - val_accuracy: 0.9405 - val_loss: 0.2559 - learning_rate: 0.0010
Epoch 6/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 44s 76ms/step - accuracy: 0.9298 - loss: 0.2947 - val_accuracy: 0.9451 - val_loss: 0.2533 - learning_rate: 0.0010
Epoch 7/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 49s 86ms/step - accuracy: 0.9423 - loss: 0.2497 - val_accuracy: 0.9173 - val_loss: 0.5001 - learning_rate: 0.0010
Epoch 8/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 44s 77ms/step - accuracy: 0.9383 - loss: 0.3080 - val_accuracy: 0.9499 - val_loss: 0.2696 - learning_rate: 0.0010
Epoch 9/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 45s 79ms/step - accuracy: 0.9724 - loss: 0.1729 - val_accuracy: 0.9724 - val_loss: 0.1324 - learning_rate: 1.0000e-04
Epoch 10/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 82s 79ms/step - accuracy: 0.9860 - loss: 0.0798 - val_accuracy: 0.9735 - val_loss: 0.1120 - learning_rate: 1.0000e-04
Epoch 11/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 45s 78ms/step - accuracy: 0.9901 - loss: 0.0613 - val_accuracy: 0.9746 - val_loss: 0.1067 - learning_rate: 1.0000e-04
Epoch 12/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 46s 80ms/step - accuracy: 0.9922 - loss: 0.0487 - val_accuracy: 0.9742 - val_loss: 0.1053 - learning_rate: 1.0000e-04
Epoch 13/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 81s 79ms/step - accuracy: 0.9956 - loss: 0.0340 - val_accuracy: 0.9770 - val_loss: 0.1085 - learning_rate: 1.0000e-04
Epoch 14/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 45s 79ms/step - accuracy: 0.9950 - loss: 0.0341 - val_accuracy: 0.9781 - val_loss: 0.1177 - learning_rate: 1.0000e-04
Epoch 15/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 81s 77ms/step - accuracy: 0.9956 - loss: 0.0302 - val_accuracy: 0.9788 - val_loss: 0.1099 - learning_rate: 1.0000e-05
Epoch 16/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 45s 79ms/step - accuracy: 0.9961 - loss: 0.0290 - val_accuracy: 0.9781 - val_loss: 0.1050 - learning_rate: 1.0000e-05
Epoch 17/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 43s 75ms/step - accuracy: 0.9962 - loss: 0.0279 - val_accuracy: 0.9779 - val_loss: 0.1075 - learning_rate: 1.0000e-05
Epoch 18/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 84s 78ms/step - accuracy: 0.9972 - loss: 0.0252 - val_accuracy: 0.9790 - val_loss: 0.1052 - learning_rate: 1.0000e-05
Epoch 19/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 44s 77ms/step - accuracy: 0.9967 - loss: 0.0276 - val_accuracy: 0.9788 - val_loss: 0.1076 - learning_rate: 1.0000e-06
Epoch 20/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 43s 75ms/step - accuracy: 0.9971 - loss: 0.0265 - val_accuracy: 0.9788 - val_loss: 0.1075 - learning_rate: 1.0000e-06
Epoch 21/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 49s 86ms/step - accuracy: 0.9979 - loss: 0.0228 - val_accuracy: 0.9788 - val_loss: 0.1064 - learning_rate: 1.0000e-07
Epoch 22/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 76s 76ms/step - accuracy: 0.9971 - loss: 0.0247 - val_accuracy: 0.9792 - val_loss: 0.1058 - learning_rate: 1.0000e-07
Epoch 23/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 44s 78ms/step - accuracy: 0.9973 - loss: 0.0246 - val_accuracy: 0.9792 - val_loss: 0.1052 - learning_rate: 1.0000e-08
Epoch 24/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 45s 79ms/step - accuracy: 0.9968 - loss: 0.0275 - val_accuracy: 0.9788 - val_loss: 0.1042 - learning_rate: 1.0000e-08
Epoch 25/25
572/572 ━━━━━━━━━━━━━━━━━━━━ 81s 76ms/step - accuracy: 0.9971 - loss: 0.0253 - val_accuracy: 0.9786 - val_loss: 0.1047 - learning_rate: 1.0000e-08

In [32] :

plot_loss_curves(history)
















In [33] :


plt.figure()
plt.plot(history.history["loss"],label="train_loss",c="dodgerblue",marker='o')
plt.plot(history.history["val_loss"],label="val_loss",c="crimson",marker='o')
plt.xlabel("Epochs")
plt.ylabel("loss")
plt.title("Loss vs Epochs")
plt.legend()


Out [33] :

<matplotlib.legend.Legend at 0x7b89d6f07520>

     









In [34] :


plt.figure()
plt.plot(history.history["accuracy"],label="train_acc",c="dodgerblue",marker="o")
plt.plot(history.history["val_accuracy"],label="val_acc",c="crimson",marker="o")
plt.xlabel("Epochs")
plt.title("Accuracy vs Epochs")
plt.ylabel("Accuracy")
plt.legend()

Out [34] :


<matplotlib.legend.Legend at 0x7b89d6f8bbb0>

















In [35] :

model.load_weights(checkpoint_path)
     
In [36] :

model.evaluate(test_data)
     
41/41 ━━━━━━━━━━━━━━━━━━━━ 8s 205ms/step - accuracy: 0.9821 - loss: 0.1224


Out [36] :

[0.10822104662656784, 0.9824561476707458]



EVOLUTION METRICS 


In [37] :


from sklearn.metrics import precision_score,accuracy_score,confusion_matrix,f1_score,roc_auc_score


y_true = []
y_pred = []

for xlabel, ylabel in test_data:
    prediction = model.predict(xlabel)
    y_pred.extend(np.argmax(prediction, axis=1))
    y_true.extend(ylabel.numpy())



1/1 ━━━━━━━━━━━━━━━━━━━━ 7s 7s/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 9s 9s/step

In [38] :

from sklearn.metrics import classification_report, recall_score, accuracy_score, precision_score, f1_score, confusion_matrix, roc_auc_score
import numpy as np

# Assuming y_true and y_pred are the ground truth and predicted labels
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average=None, zero_division=0)
f1 = f1_score(y_true, y_pred, average=None, zero_division=0)
recall = recall_score(y_true, y_pred, average=None, zero_division=0)
confusion_matrix = confusion_matrix(y_true, y_pred)

print(f"Accuracy Score: {accuracy}\n")
print(f"Precision Score: {precision}\n")
print(f"F1 Score: {f1}\n")
print(f"Recall Score: {recall}\n")
print(f"Confusion Matrix:\n{confusion_matrix}")




Accuracy Score: 0.9824561403508771

Precision Score: [0.97972973 0.95498392 1.         0.99003322]

F1 Score: [0.97315436 0.96272285 0.99752475 0.99168053]

Recall Score: [0.96666667 0.97058824 0.99506173 0.99333333]

Confusion Matrix:
[[290  10   0   0]
 [  6 297   0   3]
 [  0   2 403   0]
 [  0   2   0 298]]


In [39] :

import seaborn as sns
plt.figure()
sns.heatmap(confusion_matrix,annot=True,
           xticklabels=class_names,
           yticklabels=class_names,fmt=".0f",cmap="crest")
plt.xlabel('Predicted label')
plt.ylabel('True Label')


Out[39]:

Text(50.72222222222221, 0.5, 'True Label')
















SAVING THE MODEL

In [40] :

model.save("EfficientNetV2B2_model.keras")

In [40] :




